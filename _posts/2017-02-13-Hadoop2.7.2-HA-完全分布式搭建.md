---
published: true
layout: post
title: Hadoop2.7.2 HA 完全分布式搭建
category: Hadoop
tags: 
  - Hadoop
  - Linux
time: 2017.02.13 14:16:00
excerpt: Hadoop2.7.2 HA 完全分布式搭建
---
1. 节点设置 4个节点

******| Master | slave1 | slave2 | slave3
------|--------| -------| -------|-------
NameNode | yes | yes
DataNode | yes | yes	| yes| yes
ResourceManager | yes | yes
NodeManager | yes | yes | yes | yes 
zkfc | yes | yes
journalnode | yes | yes | yes
zookeeper | yes | yes | yes
jobhistory |||| yes

2. 虚拟机各种配置

******| 内存 | 硬盘	| ip地址
------| -----|------|------
Master | 2G | 15	|192.168.102.10
slave1 | 1G | 8 | 192.168.102.11
slave2 | 1G | 8 | 192.168.102.12
slave3 | 1G | 8	 | 192.168.102.13

3. 配置静态ip     
    ```
    vim /etc/sysconfig/network-scripts/ifcfg-enp0s3
    ```
Master 静态ip 其他类似  
![image](http://od4ghyr10.bkt.clouddn.com/image/d/b4/0ee85825bca6664b915d7eb98ffd2.png)  
可以相互ping通
设置hosts
```
    vim /etc/hosts
```
![image](http://od4ghyr10.bkt.clouddn.com/image/9/8f/fe5e7d3c452a157179e48261a982a.png)  
（1）可以在每个节点上都这么修改hosts文件  
（2）也可以有scp 源路径 目的路径  跨机器拷贝  
            则节点slave-1 上的hosts文件 内容同上。其他节点以此类推
![image](http://od4ghyr10.bkt.clouddn.com/image/2/d1/4c99b0aa1ab0a919110b9d1d3f3ca.png)

4. 修改主机名
    ```
    vim /etc/sysconfig/network
    ```
Master 的  
![image](http://od4ghyr10.bkt.clouddn.com/image/c/77/f627f8f70b6b0ffab63701dfc7086.png)  
设置完了之后hostname并没有变化。

centos7 可能有点变化   
    1.直接使用文本编辑器修改/etc/hostname配置文件。  
    2.使用hostnamectl命令，hostnamectl set-hostname name  ，再通过hostname或者hostnamectl status命令查看更改是否生效。  
执行命令  
![image](http://od4ghyr10.bkt.clouddn.com/image/0/b4/6234cb22925938d2d8afbd5305556.png)  
查看 /etc/hostname  
![image](http://od4ghyr10.bkt.clouddn.com/image/8/a8/ddec81f1d94d7cdb2a0795d1680e4.png)  
执行hostname 和 hostnamectl status 命令  
显示都为master  
其他的几个节点也这么修改  

5. 关闭防火墙  
CentOS 7.0默认使用的是firewall作为防火墙  
直接关闭防火墙  
    ```
    systemctl stop firewalld.service      //停止firewall
    systemctl disable firewalld.service //禁止firewall开机启动
    systemctl status firewalld.service //查看防火墙状态
    ```
    
6. 设置节点的免密码登录（生成私钥，公钥。。。然后分发公钥）  
```
ssh-keygen -t rsa（一路回车就可以） 
```
生成id_rsa（私钥）、id_rsa.pub（公钥）   
然后  
```
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```
 在目录~/.ssh 下  
![image](http://od4ghyr10.bkt.clouddn.com/image/2/6f/c27f0f86d3026698eefecdadf13a3.png)  

想要在那个节点免密码登录需要将自己的公钥拷贝到那个节点上  
想要节点之间相互免密码登陆需要相互拷贝公钥  
Master到其他节点。。。。  
通过命令 将master节点上的公钥拷贝到其他节点上  
ssh-copy-id -i  master 和上面命令cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 起的作用是一样的  
```
ssh-copy-id -i  slave-1
ssh-copy-id -i  slave-2
ssh-copy-id -i  slave-3
```
其他节点 执行同样的操作（生成私钥，公钥。。。然后分发公钥）  
设置其他节点的时候发现slave-3的hostname设置成了slave-4，于是一部分公钥没能分发到。。。  
需改slave-3的hostname为slave-3.。。删除master   /etc/.ssh/authorized_keys中的@slave-4代表的公钥。。（每个节点都要删除重新分发，包括slave-3自己）  
重启（可能没这个必要）。。。重新生成密钥 。重新分发  
这样每个/etc/.ssh/authorized_keys文件文件中都包含四个公钥。包括其他节点的和自己的。。。这个就可以相互访问了  
![image](http://od4ghyr10.bkt.clouddn.com/image/b/28/369b226e81342090de7dcae734a90.png)

7. 各节点安装jdk。配置环境变量  
使用winscp将jdk，hadoop；等各种包上传到master中，目录自己规定  
放到了/usr/local/目录下。。。解压  
![image](http://od4ghyr10.bkt.clouddn.com/image/3/0e/0711deffe608f357154b1637c630f.png)  
安装jdk的时候配置完环境变量并且用source /etc/profile刷新了 。。。但是系统还总是默认的是openJDK。。。  
![image](http://od4ghyr10.bkt.clouddn.com/image/2/ba/6c704f6ae3b1c408ca1d8a438b6f8.png)  
最后 找到了这个。。对centos7还是不了解
修改系统默认的JDK  
```
[Randy@localhost ~]$  sudo update-alternatives --install /usr/bin/java java /usr/lib/jdk/bin/java 300  #使系统默认的java命令是/usr/lib/jdk/bin中的java命令

[Randy@localhost ~]$  sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jdk/bin/javac 300  #使系统默认的javac命令是/usr/lib/jdk/bin中的javac命令
[Randy@localhost ~]$ sudo update-alternatives --install /usr/bin/jar jar /usr/lib/jdk/bin/jar 300 #使系统默认的jar命令是/usr/lib/jdk/bin中的jar命令 

[Randy@localhost ~]$  sudo update-alternatives --config java   #配置默认java命令
共有 3 个提供“java”的程序。
  选项    命令 
----------------------------------------------- 
*  1          /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.65-2.5.1.2.el7_0.x86_64/jre/bin/java 
  2          /usr/lib/jvm/jre-1.6.0-openjdk.x86_64/bin/java 
+ 3          /usr/lib/jdk/bin/java
按 Enter 保留当前选项[+]，或者键入选项编号：3

[Randy@localhost ~]$ sudo update-alternatives --config javac   #配置默认java命令
共有 3 个提供“javac”的程序。
  选项    命令 
----------------------------------------------- 
*  1          /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.65-2.5.1.2.el7_0.x86_64/jre/bin/javac 
  2          /usr/lib/jvm/jre-1.6.0-openjdk.x86_64/bin/javac 
+ 3          /usr/lib/jdk/bin/javac
按 Enter 保留当前选项[+]，或者键入选项编号：3
```
![image](http://od4ghyr10.bkt.clouddn.com/image/3/88/1a8fdfa8e4aac07adc1c9162bdc8f.png)  
各个节点都这样配置。。。配置完成  

8. 配置hadoop  
配置master节点上的core-site.xmlhdfs-site.xml，yarn-site.xml，mapred-site.xml配置文件  
core-site.xml为 （图片中ha.zookeeper.quorum的值应该为：master:2181,slave-1:2181,slave-2:2181）
![image](http://od4ghyr10.bkt.clouddn.com/image/b/05/667bc115136a99641671751d0b532.png)  

hdfs-site.xml
```
<configuration>
  <property>
    <name>dfs.nameservices</name>
    <value>ns1</value>
    <description>指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致</description>
  </property>
  <property>
    <name>dfs.ha.namenodes.ns1</name>
    <value>nn1,nn2</value>
    <description>ns1下有两个NameNode分别为nn1,nn2</description>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.ns1.nn1</name>
    <value>master:9000</value>
    <description>nn1的RPC通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.http-address.ns1.nn1</name>
    <value>master:50070</value>
    <description>nn1的Http通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.ns1.nn2</name>
    <value>slave-1:9000</value>
    <description>nn2的RPC通信地址</description>
  </property>
  <property>
    <name>dfs.namenode.http-address.ns1.nn2</name>
    <value>slave-2:50070</value>
    <description>nn2的Http通信地址</description>
  </property>
  <property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://master:8485;slave-1:8485;slave-2:8485/ns1</value>
       <description>指定NameNode的元数据在JournalNode上的存放位置</description>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled.ns1</name>
    <value>true</value>
   <description>true是开启NameNode失败自动切换</description>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.ns1</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    <description>配置失败自动切换实现方式 </description>
</property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/usr/local/journal</value>
     <description>指定JournalNode在本地磁盘存放数据的位置</description>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
     <description>使用sshfence隔离机制时需要ssh免登陆</description>
  </property>
  <property>
    <name>dfs.data.dir</name>
    <value>/usr/local/data</value>
  </property>
  <property>
    <name>dfs.datanode.socket.write.timeout</name>
    <value>0</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
</configuration>

yarn-site.xml
<configuration>
<!-- Site specific YARN configuration properties -->
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>                         <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>

    <name>yarn.nodemanager.local-dirs</name>

    <value>/opt/yarn/hadoop/nmdir</value>

  </property>



  <property>

    <name>yarn.nodemanager.log-dirs</name>

    <value>/opt/yarn/logs</value>

  </property>



  <property>

    <name>yarn.log-aggregation-enable</name>

    <value>true</value>

  </property> 



  <property>

    <description>Where to aggregate logs</description>

    <name>yarn.nodemanager.remote-app-log-dir</name>

    <value>hdfs://ns1/var/log/hadoop-yarn/apps</value>

  </property>



  <!-- Resource Manager Configs -->

  <property>

    <name>yarn.resourcemanager.connect.retry-interval.ms</name>

    <value>2000</value>

  </property>



  <property>

    <name>yarn.resourcemanager.ha.enabled</name>

    <value>true</value>

  </property>



  <property>

    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>

    <value>true</value>

  </property>



  <property>

    <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>

    <value>true</value>

  </property>



  <property>

    <name>yarn.resourcemanager.cluster-id</name>

    <value>ns1</value>

  </property>



  <property>

    <name>yarn.resourcemanager.ha.rm-ids</name>

    <value>rm1,rm2</value>

  </property>



  <property>

    <name>yarn.resourcemanager.ha.id</name>

    <value>rm1</value>

  </property>



  <property>

    <name>yarn.resourcemanager.scheduler.class</name>

    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>

  </property>



  <property>

    <name>yarn.resourcemanager.recovery.enabled</name>

    <value>true</value>

  </property>


  <property>

    <name>yarn.resourcemanager.zk.state-store.address</name>

    <value>master:2181,slave-1:2181,slave-2:2181</value>

  </property>



  <property>

    <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>

    <value>5000</value>

  </property>



  <!-- RM1 configs -->



  <property>

    <name>yarn.resourcemanager.address.rm1</name>

    <value>master:23140</value>

  </property>



  <property>

    <name>yarn.resourcemanager.scheduler.address.rm1</name>

    <value>master:23130</value>

  </property>



  <property>

    <name>yarn.resourcemanager.webapp.https.address.rm1</name>

    <value>master:23189</value>

  </property>



  <property>

    <name>yarn.resourcemanager.webapp.address.rm1</name>

    <value>master:23188</value>

  </property>


  <property>

    <name>yarn.resourcemanager.resource-tracker.address.rm1</name>

    <value>master:23125</value>

  </property>



  <property>

    <name>yarn.resourcemanager.admin.address.rm1</name>

    <value>master:23141</value>

  </property>



<!-- RM2 configs -->



  <property>

    <name>yarn.resourcemanager.address.rm2</name>

    <value>slave-1:23140</value>

  </property>



  <property>

    <name>yarn.resourcemanager.scheduler.address.rm2</name>

    <value>slave-1:23130</value>

  </property>



  <property>

    <name>yarn.resourcemanager.webapp.https.address.rm2</name>

    <value>slave-1:23189</value>

  </property>



  <property>

    <name>yarn.resourcemanager.webapp.address.rm2</name>

    <value>slave-1:23188</value>

  </property>



  <property>

    <name>yarn.resourcemanager.resource-tracker.address.rm2</name>

    <value>slave-1:23125</value>

  </property>



  <property>

    <name>yarn.resourcemanager.admin.address.rm2</name>

    <value>slave-1:23141</value>



  </property>



<!-- Node Manager Configs -->

  <property>

    <description>Address where the localizer IPC is.</description>

    <name>yarn.nodemanager.localizer.address</name>

    <value>0.0.0.0:23344</value>

  </property>



  <property>

    <description>NM Webapp address.</description>

    <name>yarn.nodemanager.webapp.address</name>

    <value>0.0.0.0:23999</value>

  </property>



  <property>

    <name>yarn.nodemanager.local-dirs</name>

    <value>/opt/yarn/nodemanager/yarn/local</value>

  </property>



  <property>

    <name>yarn.nodemanager.log-dirs</name>

    <value>/opt/yarn/nodemanager/yarn/log</value>

  </property>



  <property>

    <name>mapreduce.shuffle.port</name>

    <value>23080</value>

  </property>



  <property>

    <name>yarn.resourcemanager.zk-address</name>

    <value>master:2181,slave-1:2181,slave-2:2181</value>

  </property>
</configuration>
```

mapred-site.xml(一开始是个template的文件，改下文件名)  
```
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<!-- configure historyserver -->
<property>
<name>mapreduce.jobhistory.address</name>
<value>slave-4:10020</value>
    <!--<value>hadoop4:10020</value> 一个开始这里设置错误了导致了jobhistory无法访问，由于slave-3为未修复，所以用slave-4作为日志节点-->
</property>
<property>
<name>mapreduce.jobhistory.webapp.address</name>
<value>slave-4:10020</value>
    <!--<value>hadoop4:19888</value>--这里导致的错误同上-->
</property><property>
<name>mapred.job.reuse.jvm.num.tasks</name>
<value>-1</value>
</property>
<property>
<name>mapreduce.reduce.shuffle.parallelcopies</name>
<value>20</value>
</property>
</configuration>
```

使用命令
```
scp -r /usr/local/hadoop2.7.2   root@slave：/usr/local/hadoop2.7.2
```
将hadoop2.7.2安装包配置文件等全部传输到其他节点上



9. zk的设置，先将zk安装目录下conf/zoo_sample.cfg重命名zoo.cfg，然后修改配置  
```
# The number of milliseconds of each tick
#服务器与客户端之间交互的基本时间单元
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
#zk能接受的客户端的数量
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
#服务器和客户端之间的时间间隔
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
#保存zk日志和数据的目录路径，（此处修改了）
dataDir=/usr/local/zookeeper3.4.8/zookeeper_data_log
# the port at which the clients will connect
#客户端与zk的交互端口
clientPort=2181
（下面是在查看状态为standalone之后加的内容）
server.1= master:2888:3888 
server.2= slave-1:2888:3888 
server.3= slave-2:2888:3888
#server.A=B:C:D  其中A是一个数字，代表这是第几号服务器；B是服务器的IP地址；
#C表示服务器与群集中的“领导者”交换信息的端口；当领导者失效后，
#D表示用来执行选举时服务器相互通信的端口。

# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
```
zk的配置文件可以每个节点分别改。也可以使用命令scp传输给其他节点
启动zk  
![image](http://od4ghyr10.bkt.clouddn.com/image/5/8b/f55770fc8f0f2e2e523bea91cdd85.png)  
但是启动了三个配置zk的节点查看状态都是  
![image](http://od4ghyr10.bkt.clouddn.com/image/f/c7/dd3810e58f536365dd1f4771a8a66.png)  
不应该是这样的  
在zoo.cfg配置文件中添加  
```
server.1= master:2888:3888 
server.2= slave-1:2888:3888 
server.3= slave-2:2888:3888
```
然后再每个节点中在刚才设置的目录
dataDir=/usr/local/zookeeper3.4.8/zookeeper_data_log
中新建已给myid的文件添加相应得数字，里面写入一个0-255之间的一个随意数字，每个zk上这个文件的数字要是不一样的，这些数字应该是从1开始，依次写每个服务器。文件中序号要与dn节点下的zk配置序号一直，比如master节点中应该为1.  
![image](http://od4ghyr10.bkt.clouddn.com/image/9/f3/6b00efbb2233f41a494c0d42b6318.png)

然后再次启动zk，（好像启动 顺序会有一些影响，但是具体影响哪里，怎么影响还不太清楚）
查看状态  
master  
![image](http://od4ghyr10.bkt.clouddn.com/image/8/ef/2fd3255c0fe492a8a5019ad917f95.png)  
slave-1  
![image](http://od4ghyr10.bkt.clouddn.com/image/c/64/2a2d71d90b4e8949f944f1e6f0de8.png)  
slave-2  
![image](http://od4ghyr10.bkt.clouddn.com/image/d/84/de76c2b6c1311b2a9b21042524b8e.png)

其中的leader是zk投票选出来的。  
命令jps  
![image](http://od4ghyr10.bkt.clouddn.com/image/c/20/dd6f440e2a979db08965929ce7c5b.png)

接下来启动journalnode (master,slave-1,slave-2节点上执行)  
命令hadoop-daemon.sh start journalnode  
![image](http://od4ghyr10.bkt.clouddn.com/image/8/7a/8a4e8fd6dfd50ffc92f51ec4188eb.png)  
接下来在节点master上格式化HDFS
命令hdfs namenode -format

格式化zk  
命令hdfs zkfc -formatZK  

然后启动master节点上的namenode和zkfc  
命令分别为hadoop-daemon.sh start namenode   和 hadoop-daemon.sh start zkfc  
启动之后jps查看  
![image](http://od4ghyr10.bkt.clouddn.com/image/c/94/a55c35860e12208987e715b10424a.png)  


在节点slave-1上数据同步格式化的master上的hdfs
命令： hdfs namenode -bootstrapStandby
然后同master一样启动namenode和zkfc
启动namenode的时候总是出错查看日志显示是50070端口的问题。。。。查看hdfs-site.xml文件发现slave-1写成slave-2了
![image](http://od4ghyr10.bkt.clouddn.com/image/b/e1/ea7dd0686b489e7b97e6c525e70b8.png)
修改之后可以了  
![image](http://od4ghyr10.bkt.clouddn.com/image/a/5c/1af52418c31c568f00fcb9a5dd40d.png)  
然后
在master节点上启动HDFS  
start-dfs.sh 出错 但是/etc/profile中都配置了  
![image](http://od4ghyr10.bkt.clouddn.com/image/f/dc/835d23465f2582a32d64a3f06d2df.png)  

修改了各个节点hadoop-env-sh文件  
JAVA_HOME=/usr/local/jdk1.8  

master,slave-1,slave-2,都可以了，slave-3由于之前没有配置还有问题  
![image](http://od4ghyr10.bkt.clouddn.com/image/9/8a/34a84b27bdce0a2aa60d502178033.png)  
将master中的hadoop2.7.2 全部通过scp命令传给slave-3即可  
```
scp -r /usr/local/hadoop2.7.2  root@slave-3:/usr/local/
```
再次启动  
![image](http://od4ghyr10.bkt.clouddn.com/image/7/14/a2434154b3ff0e7ac20697afa5901.png)


启动yarn  
先配置yarn-env.sh
```
JAVA_HOME=/usr/local/jdk1.8
```
执行命令启动
start-yarn.sh  这样应该是算好了
![image](http://od4ghyr10.bkt.clouddn.com/image/b/76/fff72af44627d27088a6c68e316d1.png)
slave-3上启动 JobHistoryServer  
```
执行命令 mr-jobhistory-daemon.sh start historyserver
```

应该验证一些功能  
namenode master  192.168.102.10:50070/  
![image](http://od4ghyr10.bkt.clouddn.com/image/f/40/e629a786d8c79bb3c11da114bd215.png)  
namenode slave-1  192.168.102.11:50070/  
结果和上图差不多  

http://192.168.102.13:19888/jobhistory  
slave-3 上的jobHistory  
![image](http://od4ghyr10.bkt.clouddn.com/image/a/b9/e0a3dc1d382185736d17ab64d0738.png)

http://192.168.102.11:8088/  。。。。Allapplication没能成功


1. 验证hdfs HA  
首先向hdfs上传一个文件：    hadoop fs -put /usr/local/ /testHA  
然后查看： hadoop fs -ls /  
![image](http://od4ghyr10.bkt.clouddn.com/image/3/fd/ea2fae6ce746c27a6fd19817ab9db.png)
kill掉active的NameNode。  
![image](http://od4ghyr10.bkt.clouddn.com/image/b/d6/0a8bbb28ead98d159660338c08dd6.png)  
然后浏览器访问 看到  slave-1变成active的了。  
![image](http://od4ghyr10.bkt.clouddn.com/image/2/ef/be4720c064ef6571652424cb21a02.png)  
在执行命令：hadoop fs -ls /  
文件还在。  
![image](http://od4ghyr10.bkt.clouddn.com/image/d/82/a5603e1382774fde85efa27c83a1d.png)  
然后再启动刚才停掉的namenode 。然后访问，变成standby的了。

安装scala。。。和java一样下载，解压配置环境变量，验证  
![image](http://od4ghyr10.bkt.clouddn.com/image/6/57/beafcf1bb2307a2e82258f210372e.png)  
然后将scala安装文件和环境变量配置文件/etc/profile（也可以一个个节点修改）传给其他节点  
配置完/etc/profile文件需要用source /etc/profile 刷新配置  
部署spark下载解压到自己设置的目录上  
然后将spark安装包中conf文件夹下的spark-env.sh.template重命名为spark-env.sh 然后添加配置  
```
export JAVA_HOME=/usr/local/jdk1.8
export SCALA_HOME=/usr/local/scala2.12.0.M4
export HADOOP_CONF_DIR=/usr/local/hadoop2.7.2/etc/hadoop
export SPARK_MASTER_IP=192.168.102.13
export SPARK_WORKER_MEMORY=512m
```
然后将配置完成的spark安装包分发给各个节点  
将slaves.temple 改成slaves  
然后添加（slave-3是spark集群的master）  
```
slave-1
slave-2
slave-4
```
在slave-3上启动集群  
启动master： sbin/start-master.sh  
启动salve： sbin/start-slaves.sh  
http://192.168.102.13:8080/  

各个节点配置完成  
启动dfs，yarn，zk。。  

1.启动zookeeper。。。节点master slave-1 slave-2   都启动，启动之后应该有一个leader两个follower  
 命令： zkServer.sh start   zkServer.sh status  
2.启动JournalNode。。。节点master slave-1 slave-2 都启动  
命令：hadoop-daemon.sh start journalnode  
3.启动master的namenode和zkfc  
命令：hadoop-daemon.sh start namenode   和 hadoop-daemon.sh start zkfc  
4.在slave-1和2上同样启动namenode和zkfc  
5.在masters上启动HDFS,启动yarn  
命令：start-dfs.sh 和 start-yarn.sh  
此时master上的jps 应该如下  
![image](http://od4ghyr10.bkt.clouddn.com/image/8/ae/e4e5f064c84ce4fc488b51794263d.png)  
6.在slave-3上启动JobHistoryServer  
命令： mr-jobhistory-daemon.sh start historyserver  

启动方式2  
1.启动zookeeper。。。节点master slave-1 slave-2   都启动，启动之后应该有一个leader两个follower  
 命令： zkServer.sh start   zkServer.sh status  
2.启动master,slave-1,slave-2上的zkfc  
hadoop-daemon.sh start zkfc  
3.在master上启动hadoop集群  
start-dfs.hs,start-yarn.sh  (使用start-all.sh命令会漏掉ResourceManager)  
4.在slave-3上启动JobHistoryServer  
命令： mr-jobhistory-daemon.sh start historyserver  
5.启动hbase集群  
start-hbase.sh

启动顺序注意：  
在集群启动时，需注意按照以下顺序  
　　第一步：由于我们选择的是QJM方案，需要使用到zookeeper，所以在各个DataNode节点上启动zookeeper服务  
　　第二步：在其中一台NameNode节点（这里我预选取的是NameNode Active节点）启动journalnode服务，该服务用于共享存储，同步节点信息。  
　　第三步：若是首次启动，需要在其中一台NameNode Active节点上格式HDFS  
　　第四步：接着我们同样一台NameNode Active节点格式化zkfc,它对应的类是DFSZKFailoverController  
　　第五步：在NameNode Active节点启动hdfs服务和yarn服务  
　　第六步：同步NameNode Active节点的元数据  

hdfs dfsadmin -safemode leave：关闭safemode

上传training_set大约65min

启动spark 到spark的bin目录下 ./start-all.sh  出错。。。。初步判断应该是版本对应的问题
hadoop scala   spark的版本对应问题还真是烦啊！！！  
![image](http://od4ghyr10.bkt.clouddn.com/image/a/7f/cf6ccc430049a9d03b8fe32cf69f2.png)  
![image](http://od4ghyr10.bkt.clouddn.com/image/5/4b/ab0b8aa73ae9f4e7da52ad6f98ff7.png)



配置hbase1.2.1
将压缩包上传到了master上，解压配置环境变量 记得向其他几个节点同步环境变量  
![image](http://od4ghyr10.bkt.clouddn.com/image/d/bf/90ddb941c976d4c98e034c7f379f8.png)

修改配置文件
hbase-env.sh
```
export JAVA_HOME=/usr/local/jdk1.8

export HBASE_CLASSPATH=/usr/local/hbase1.2.1/conf

export HBASE_MANAGES_ZK=false
```
修改文件regionservers
添加自己节点的主机名  
![image](http://od4ghyr10.bkt.clouddn.com/image/7/ff/9fce399f7853cfd9cd23f23b96f30.png)  
修改hbase-site.xml
```
<?xml version="1.0"?>

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>

<!--因为是多台master,所以hbase.roodir的值跟hadoop配置文件hdfs-site.xml中dfs.nameservices的值是一样的-->

<name>hbase.rootdir</name>

<value>hdfs://ns1/hbase</value>

</property>

<property>
<!--启动分布式模式--> 
<name>hbase.cluster.distributed</name>

<value>true</value>

</property>

<property>

<name>hbase.tmp.dir</name>

<value>/usr/local/tmpdir/hbasetmp</value> 

</property>

<property>

<name>hbase.zookeeper.quorum</name>

<value>Master,Slave-1,Slave-2,Slave-3</value> 

</property>

<property>

<!--当定义多台master的时候，我们只需要提供端口号-->

<name>hbase.master.port</name>

<value>60000</value>

</property>

<property>

<name>zookeeper.session.timeout</name>

<value>60000</value>

</property>

<property>

<name>hbase.zookeeper.property.clientPort</name>

<value>2181</value>

</property>

<property>

<!--跟zookeeperper配置的dataDir一致-->

<name>hbase.zookeeper.property.dataDir</name>

<value>/usr/local/zookeeper3.4.8/zookeeper_data_log</value> 

</property>

</configuration>
```
使用scp命令同步配置好的Hbase到其他节点上  
![image](http://od4ghyr10.bkt.clouddn.com/image/e/3c/cfa4e646077111b26304958e5d03b.png)  

启动Hbase  start-hbase.sh  
先启动Hadoop集群再启动Hbase  
一开始启动只有master的Hmaster启动了其他节的HRegionServer都没有启动，（Hbase的HA是基于NameNode的HA的）  
日志的提示错误是：未知的hostname：ns1  
HBase做HA和不做HAhbase-site.xml的配置是不一样的  
后来将每个节点上的hdfs-site.xml和core-site.xml配置文件复制一份到hbase的conf目录下，再次启动启动就正常了  
但是这样配置之后，NameNode的HA和Hbase的HA应该会有一定的影响，不过具体什么影响目前还不知道。。。但是要记住这个问题！！！  


集群同时时间设置  
每台机器上：  
命令查看rpm  -q ntp 查看是否安装了ntp  
如果没有安装则yum install ntp 安装

配置ntp服务自启动   
chkconfig ntpd on  

查看一下是系统的时间  
命令：date  
如果时间差太多就同步一下时间  
ntpdate 210.72.145.44     #ip地址为国家授时服务器的  

更改配置文件  
vim /etc/ntp.conf
```
# For more information about this file, see the man pages
# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).

driftfile /var/lib/ntp/drift

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict ::1
restrict 192.168.102.11 mask 255.255.255.0 nomodify notrap
restrict 192.168.102.12 mask 255.255.255.0 nomodify notrap
restrict 192.168.102.13 mask 255.255.255.0 nomodify notrap
# Hosts on local network are less restricted.
restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst
server 210.72.145.44 perfer   # 中国国家受时中心
server 202.112.10.36             # 1.cn.pool.ntp.org
server 59.124.196.83             # 0.asia.pool.ntp.org
#broadcast 192.168.1.255 autokey        # broadcast server
#broadcastclient                        # broadcast client
#broadcast 224.0.1.1 autokey            # multicast server
#multicastclient 224.0.1.1              # multicast client
#manycastserver 239.255.254.254         # manycast server
#manycastclient 239.255.254.254 autokey # manycast client
# allow update time by the upper server 
# # 允许上层时间服务器主动修改本机时间
restrict 210.72.145.44 nomodify notrap noquery
restrict 202.112.10.36 nomodify notrap noquery
restrict 59.124.196.83 nomodify notrap noquery
Enable public key cryptography.
## 外部时间服务器不可用时，以本地时间作为时间服务
server  127.127.1.0     # local clock
fudge   127.127.1.0 stratum 10
#crypto

includefile /etc/ntp/crypto/pw

# Key file containing the keys and key identifiers used when operating
# with symmetric key cryptography. 
keys /etc/ntp/keys

# Specify the key identifiers which are trusted.
#trustedkey 4 8 42

# Specify the key identifier to use with the ntpdc utility.
#requestkey 8

# Specify the key identifier to use with the ntpq utility.
#controlkey 8

# Enable writing of statistics records.
#statistics clockstats cryptostats loopstats peerstats

# Disable the monitoring facility to prevent amplification attacks using ntpdc
# monlist command when default restrict does not include the noquery flag. See
# CVE-2013-5211 for more details.
# Note: Monitoring will not be disabled with the limited restriction flag.
disable monitor 
```
启动服务  
systemctl start  ntpd.service

查看  
netstat -tlunp | grep ntp

 其他节点同步时间  
ntpdate  master（master为刚才配置的机器名）

定时执行时间同步  
在其他节点上crontab -e  
然后，添加 ，这里设置类一小时执行一次时间同步 
```
* */1 * * * /usr/sbin/ntpdate 192.168.102.10
```

安装Flume（未完成）  
大致的架构是slave-1和slave-2监控特定目录下的文件变化，然后传给slave-3最后由slave-3上传到HDFS中  
首先配置环境变量  
![image](http://od4ghyr10.bkt.clouddn.com/image/b/cd/b04b58628cb5d8e52c43d7dc43276.png)

然后再flume1.6.0/conf中配置 
在flume-env.sh中添加  
![image](http://od4ghyr10.bkt.clouddn.com/image/2/73/38d833b23af502d44e08441944b53.png)  
没有这个文件则改名字  
然后分发到另外两个节点中  
 在slave-3节点上,在flume conf目录中flume-conf.properties.template重命名为agent0.conf  
修改配置  

安装配置Hive  
Hive版本：hive2.1.0  
1.在/etc/profile中配置环境变量  
```
export JAVA_HOME=/usr/local/jdk1.8
export HADOOP_HOME=/usr/local/hadoop2.7.2
export SCALA_HOME=/usr/local/scala2.10.6
export FLUME_HOME=/usr/local/flume1.6.0
export SPARK_HOME=/usr/local/spark1.6.1
export ZK_HOME=/usr/local/zookeeper3.4.8
export HBASE_HOME=/usr/local/hbase1.2.1
export HIVE_HOME=/usr/local/hive2.1.0
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$SCALA_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$FLUME_HOME/bin:$HIVE_HOME/bin
export CLASSPATH=.:%JAVA_HOME%/lib/dt.jar:%JAVA_HOME%/lib/tools.jar:$HIVE_HOME/lib
```

2.进入到./hive2.1.0/conf目录中  
hive-default.xml.template重命名为hive-default.xml  
hive-env.sh.template重命名为hive-env.sh  
hive-exec-log4j2.properties.template重命名为hive-exec-log4j2.properties   
新建hive-site.xml文件  
在hive-env.sh中配置  
```
HADOOP_HOME=/usr/local/hadoop2.7.2
export HIVE_CONF_DIR=/usr/local/hive2.1.0/conf
```
在hive-site.xml中配置  
```
<configuration>
<!--HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。 -->
<property>
  <name>hive.exec.scratchdir</name>
  <value>/tmp</value>
</property>
<!-- 日志的记录位置-->
<property>
<name>hive.querylog.location</name>
<value>/usr/local/hive2.1.0/hivelogs</value>
</property>
<!--在hdfs上hive数据存放目录，启动hadoop后需要在hdfs上手动创建-->
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
  </property>
<!--通过jdbc协议连接mysql的hive库-->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://slave-4:3306/hiveDB?createDatabaseIfNotExist=true</value>
  </property>

<!--jdbc的mysql驱动-->
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>

<!--mysql用户名-->
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
  </property>
<!--mysql用户密码-->
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>1992</value>
  </property>

<!--hive的web页面-->
  <property>
    <name>hive.hwi.war.file</name>
    <value>lib/hive-hwi-2.1.0.war</value>
  </property>

<!--指定hive元数据访问路径,可以有多个，逗号分隔-->
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://192.168.102.14:9083</value>
</property>
<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.autoCreateTables</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.autoCreateColumns</name>
  <value>true</value>
</property>

<!--hiveserver2的HA--暂时不配置>
 <-- <property>
    <name>hive.zookeeper.quorum</name>
    <value>master,slave-1,slave-2</value>
  </property> -->
</configuration>
```
在mysql中建立hive-site.xml中指定的用户名，密码，hiveDB数据库  
在HDFS中建立hive-site.xml中指定的目录/tmp和/user/hive/warehouse并修改权限为775  
下载hive2.1.0的源代码进入到hwi/web 中执行 jar cvf hive-hwi-1.1.0.war ./*   然后将生成的 
hive-hwi-1.1.0.war 移动到hive2.1.0/lib下面  
将/etc/profile和hive2.1.0下文件复制到其他节点  
执行命令
```
 hive --service metastore &
hive --service hiveserver2 &
hive
 ```
进入hive



