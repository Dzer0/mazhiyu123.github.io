---
layout:     post
random-img: true
title:      Azkaban创建工作流
date:       2017-08-16 17:33:00
author:     mazhiyu
description: Azkaban创建工作流
keywords: Azkaban
tags:
    - Azkaban
---



#### 关键字type

type有四种类型：java，command，javaprocess和pig


#### 定义开始节点start_flow.job


```
type=command
command=echo "Start Work Flow" 
```

#### 定义流程中处理任务的节点

如sync_upload_data_from_host.job

```
type=command
dependencies=start_flow
command=sh ../job_scriptes/sync_upload_data_from_host.sh 
```

然后在../job_scriptes/这个路径下定义sync_upload_data_from_host.sh脚本

```
#!/bin/bash
basedir=$(cd "$(dirname "$0")"; pwd)
source $basedir/common.sh

/usr/bin/expect << EOF
set time 30
spawn ssh -p22 $vps_ssh_user@$vps_host
expect {
"*password:" { send "$vps_ssh_pwd\r" }
}
expect "#*"
send "cd $vps_upload_path$dateymd\r"
send "find ./ -ctime -1 -exec tar czvf /tmp/upload_data_$dateymd.tar.gz {} \\\;\r"

spawn scp $vps_ssh_user@$vps_host:/tmp/upload_data_$dateymd.tar.gz $cluster_upload_path 
expect {
"*password:" { send "$vps_ssh_pwd\r" }
}
send "exit\r "
expect eof
EOF

# 解压上传的文件
mkdir -p $cluster_upload_path$dateymd
tar -zxvf $cluster_upload_path"upload_data_"$dateymd.tar.gz -C $cluster_upload_path$dateymd
rm -rf $cluster_upload_path"upload_data_"$dateymd.tar.gz
```

节点中的其他任务节点的定义都是类似的方式，由.job文件控制任务流程的依赖关系，和启动命令等。


#### 定义结束节点end_flow.job

```
type=command
dependencies=sync_get_in_data_to_host,put_to_hbase,put_to_hive
command=echo  "End Work Flow"
```

其中dependencies需要添加所有的没有子任务的任务节点， 否则会出错。

#### 上传到服务器

将包含.job文件的文件夹，压缩打包然后上传到服务器中，就可以看到定义的工作流

![image](http://od4ghyr10.bkt.clouddn.com/azkaban/sync_data%E5%B7%A5%E4%BD%9C%E6%B5%81.png)


