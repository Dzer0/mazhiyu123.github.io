---
published: true
layout: post
title: MRApp异常情况1
category: Hadoop
tags: 
  - Hadoop
time: 2017.04.13 11:16:00
excerpt:   MRApp异常情况1
---


#### Container超过虚拟内存使用限制

```
Job job_1491843501589_0003 running in uber mode : false
2017-04-10 20:03:48,692 INFO mapreduce.Job:  map 0% reduce 0%
2017-04-10 20:04:12,664 INFO mapreduce.Job: Task Id : attempt_1491843501589_0003_m_000005_0, Status : FAILED
Container [pid=54514,containerID=container_e01_1491843501589_0003_01_000005] is running beyond virtual memory limits. Current usage: 54.0 MB of 1 GB physical memory used; 2.3 GB of 2.1 GB virtual memory used. Killing container.
Dump of the process-tree for container_e01_1491843501589_0003_01_000005 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 54520 54514 54514 54514 (java) 161 101 2464305152 13520 /usr/cluster/java/jdk1.8.0_121/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx820m -Djava.io.tmpdir=/usr/cluster/hadoop3.0-alpha2/yarn/nmlocaldir/usercache/root/appcache/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/cluster/hadoop3.0-alpha2/yarn/nmlogdirs/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.102.12 58503 attempt_1491843501589_0003_m_000005_0 1099511627781 
	|- 54514 54513 54514 54514 (bash) 0 0 9744384 292 /bin/bash -c /usr/cluster/java/jdk1.8.0_121/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN   -Xmx820m -Djava.io.tmpdir=/usr/cluster/hadoop3.0-alpha2/yarn/nmlocaldir/usercache/root/appcache/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/cluster/hadoop3.0-alpha2/yarn/nmlogdirs/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.102.12 58503 attempt_1491843501589_0003_m_000005_0 1099511627781 1>/usr/cluster/hadoop3.0-alpha2/yarn/nmlogdirs/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005/stdout 2>/usr/cluster/hadoop3.0-alpha2/yarn/nmlogdirs/application_1491843501589_0003/container_e01_1491843501589_0003_01_000005/stderr  

Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143. 

```

在yarn-site.xml中配置了如下参数,错误消除（第三个起到了作用）

```
<!-- 该节点上Yarn可使用的物理内存总量 -->  
  <property>  
    <name>yarn.nodemanager.resource.memory-mb</name>  
    <value>1536</value>  
  </property>  

  <!-- 单个任务可申请的最多物理内存 -->  
  <property>  
    <name>yarn.scheduler.maximum-allocation-mb</name>  
    <value>1536</value>  
  </property> 
  
  <!-- 任务每使用1MB物理内存，最多可使用的虚拟内存量 -->    
  <property>  
    <name>yarn.nodemanager.vmem-pmem-ratio</name>  
    <value>3</value>  
  </property>  

```

### 不可用的资源请求（请求内存的资源大于内存上限）

mapreduce默认需要的内存为1536M，分配的过小
在mapred-site.xml中yarn.app.mapreduce.am.resource.mb参数的默认值是1536M。
需要使yarn-site.xml中yarn.scheduler.maximum-allocation-mb的值大于mapred-site.xml中yarn.app.mapreduce.am.resource.mb的值

```
number of splits:19
2017-04-11 19:25:28,328 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2017-04-11 19:25:28,827 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1491963845428_0001
2017-04-11 19:25:29,381 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1491963845428_0001
java.io.IOException: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=1536, maxMemory=1024
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:253)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:218)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:189)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:453)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:359)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:298)
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:643)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:262)

```

### Shuffle阶段，对内存溢出


```
2017-04-11 23:00:46,158 INFO [IPC Server handler 15 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1491976524024_0001_r_000000_0. startIndex 0 maxEvents 10000
2017-04-11 23:00:47,265 INFO [IPC Server handler 29 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1491976524024_0001_r_000000_0. startIndex 11 maxEvents 10000
2017-04-11 23:00:48,685 INFO [IPC Server handler 9 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: MapCompletionEvents request from attempt_1491976524024_0001_r_000000_0. startIndex 11 maxEvents 10000
2017-04-11 23:00:49,120 INFO [IPC Server handler 5 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1491976524024_0001_m_000011_0 is : 0.51261073
2017-04-11 23:00:49,379 INFO [IPC Server handler 24 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1491976524024_0001_r_000000_0 is : 0.0
2017-04-11 23:00:49,627 FATAL [IPC Server handler 9 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1491976524024_0001_r_000000_0 - exited : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:171)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:59)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:312)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:302)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:538)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:348)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198)

2017-04-11 23:00:49,628 INFO [IPC Server handler 9 on 36102] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1491976524024_0001_r_000000_0: Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#5
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:171)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:59)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:312)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:302)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:538)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:348)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198)

```

配置了两个参数，第二个参数起了作用了

```
<!-- 开始merger的百分比--> 
  <property>
    <name>mapreduce.reduce.shuffle.merge.percent</name>
    <value>0.5</value>
  </property>

<!-- 放到内存还是磁盘上的阈值--> 
  <property>
    <name>mapreduce.reduce.shuffle.memory.limit.percent</name>
    <value>0.15</value>
  </property>


```