---
published: true
layout: post
title: Hadoop3.0-alpha2 集群搭建 HA
category: Hadoop
tags: 
  - Hadoop
time: 2017.04.12 22:49:00
excerpt:   shell 管理Linux操作系统
---

1. 节点信息

 节点名称 |  内存 | 磁盘 | ip地址
 ---------| ----- | ---- | -----
 master1  | 1G | 30G | 192.168.102.10
 master2  | 1G | 30G | 192.168.102.11
 slave1  | 1G | 70G | 192.168.102.12
 slave2  | 1G | 70G | 192.168.102.13
 slave3  | 1G | 70G | 192.168.102.14
 
2. 各个节点承担的职责

*** | master1 | master2 | slave1 | slave2 | slave3
----|---------|---------|--------|--------|-------
NameNode | yes | yes |
jobhistory | | yes |
ResourceManager | yes | yes
zookeeper | yes | yes | yes 
zkfc | yes | yes |
DataNode | | | yes | yes |yes 


## 部署集群前虚拟机的配置准备
 
 1. 配置静态IP
 2. 设置/etc/hosts
 3. 修改hostname
 4. 关闭防火墙
 5. 设置SSH免密登陆  
 6. 配置Java环境
 以上配置和[](http://mazhiyu.info/hadoop/2017/02/13/Hadoop2.7.2-HA-%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA)
 7. 关闭selinux （/etc/selinux/config中SELINUX的值设置为disabled，重启生效） 
 8. 指定DNS
    ```
    [root@master1 network-scripts]# nmcli connection show
    NAME                UUID                                  TYPE            DEVICE      
    virbr0              5a372876-c1d0-482c-94d9-c0c136842e95  bridge          virbr0      
    Wired connection 1  99d4e0c1-0c42-4490-8eaf-fb1d1015eff1  802-3-ethernet  --          
    eno16777736         3948e27e-c12f-443d-8ddb-b3d47b1bd4d5  802-3-ethernet  eno16777736 
    [root@master1 network-scripts]# nmcli con mod eno16777736 ipv4.dns "114.114.114.114 8.8.8.8"
    [root@master1 network-scripts]# nmcli con up eno16777736
    Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)
    [root@master1 network-scripts]# ping www.baidu.com
    PING www.baidu.com (119.75.217.109) 56(84) bytes of data.
    64 bytes from 119.75.217.109: icmp_seq=1 ttl=128 time=23.2 ms

    ```
 9. master1 安装mysql
 

## 部署zookeeper集群

配置zookeeper环境变量
```
export JAVA_HOME=/usr/cluster/java/jdk1.8.0_121
export JRE_HOME=/usr/cluster/java/jdk1.8.0_121/jre
export ZK_HOME=/usr/cluster/zookeeper3.4.10/zookeeper-3.4.10
export PATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

master1节点上进行对zoo.cfg配置

```
# The number of milliseconds of each tick
#服务器与客户端之间交互的基本时间单元
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
#zk能接受的客户端的数量
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
#服务器和客户端之间的时间间隔
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
#保存zk日志和数据的目录路径，（此处修改了）
dataDir=/usr/cluster/zookeeper3.4.10/data_log
# the port at which the clients will connect
#客户端与zk的交互端口
clientPort=2181
#（下面是在查看状态为standalone之后加的内容）
server.1= master1:2888:3888 
server.2= master2:2888:3888 
server.3= slave1:2888:3888
#server.A=B:C:D  其中A是一个数字，代表这是第几号服务器；B是服务器的IP地址；
#C表示服务器与群集中的“领导者”交换信息的端口；当领导者失效后，
#D表示用来执行选举时服务器相互通信的端口。

# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
```

创建zoo.cfg中的dataDir的目录data_log，然后创建文本myid写入0-225之间的数字，从1开始

```
mkdir data_log
cd data_log
touch myid
echo 1 > myid
```

将配置好的zookeeper发送到其他节点（master2，slave1),修改myid分别为2和3.
将修改后的环境变量/etc/profile同样发送到其他节点

zkServer.sh start 启动
zkServer.sh status 查看状态
两个follower，一个leader

完成

## 部署Hadoop2.7.2

配置hadoop环境变量

```
export JAVA_HOME=/usr/cluster/java/jdk1.8.0_121
export JRE_HOME=/usr/cluster/java/jdk1.8.0_121/jre
export ZK_HOME=/usr/cluster/zookeeper3.4.10/zookeeper-3.4.10
export HADOOP_HOME=/usr/cluster/hadoop3.0-alpha2/hadoop-3.0.0-alpha2
export PATH=$PATH:$JAVA_HOME/bin:$ZK_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar

```
配置workers

```
slave1
slave2
slave3
```

配置hadoop-env.sh

```
export JAVA_HOME=/usr/cluster/java/jdk1.8.0_121
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_JOURNALNODE_USER=root

```

#### 配置core-site.xml

```

<configuration>

<!-- 指定hdfs的nameservice为ns -->  
 <property>      
      <name>fs.defaultFS</name>      
      <value>hdfs://ns1</value>      
 </property>  

 <!--指定hadoop数据临时存放目录-->  
 <property>  
      <name>hadoop.tmp.dir</name>  
      <value>/usr/cluster/hadoop3.0-alpha2/datatmp</value>  
 </property>     
                            
 <property>      
      <name>io.file.buffer.size</name>      
      <value>4096</value>      
 </property>  

 <!--指定zookeeper地址-->  
 <property>  
      <name>ha.zookeeper.quorum</name>  
      <value>master1:2181,master2:2181,slave1:2181</value>  
 </property>  
</configuration>

```

#### 配置mapred-site.xml

```

<configuration>

<!-- 指定hdfs的nameservice为ns -->  
 <property>      
      <name>fs.defaultFS</name>      
      <value>hdfs://ns1</value>      
 </property>  

 <!--指定hadoop数据临时存放目录-->  
 <property>  
      <name>hadoop.tmp.dir</name>  
      <value>/usr/cluster/hadoop3.0-alpha2/datatmp</value>  
 </property>     
                            
 <property>      
      <name>io.file.buffer.size</name>      
      <value>4096</value>      
 </property>  

 <!--指定zookeeper地址-->  
 <property>  
      <name>ha.zookeeper.quorum</name>  
      <value>master1:2181,master2:2181,slave1:2181</value>  
 </property>  
</configuration>

```


#### 配置hdfs-site.xml

```

<configuration>
  
  <property>
    <name>dfs.nameservices</name>
    <value>ns1</value>
    <description>指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致</description>
  </property>
  
  <property>
    <name>dfs.ha.namenodes.ns1</name>
    <value>nn1,nn2</value>
    <description>ns1下有两个NameNode分别为nn1,nn2</description>
  </property>
 
   <property>
    <name>dfs.namenode.rpc-address.ns1.nn1</name>
    <value>master1:9000</value>
    <description>nn1的RPC通信地址</description>
   </property>
  
  <property>
    <name>dfs.namenode.http-address.ns1.nn1</name>
    <value>master1:50070</value>
    <description>nn1的Http通信地址</description>
  </property>
  
  <property>
    <name>dfs.namenode.rpc-address.ns1.nn2</name>
    <value>master2:9000</value>
    <description>nn2的RPC通信地址</description>
  </property>

  <property>
    <name>dfs.namenode.http-address.ns1.nn2</name>
    <value>master2:50070</value>
    <description>nn2的Http通信地址</description>
  </property>

  <property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://master1:8485;master2:8485;slave1:8485/ns1</value>
       <description>指定NameNode的元数据在JournalNode上的存放位置</description>
  </property>
  
  <property>
    <name>dfs.ha.automatic-failover.enabled.ns1</name>
    <value>true</value>
   <description>true是开启NameNode失败自动切换</description>
  </property>
  
  <property>
    <name>dfs.client.failover.proxy.provider.ns1</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    <description>配置失败自动切换实现方式 </description>
  </property>
  
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/usr/cluster/hadoop3.0-alpha2/journalnodedata</value>
     <description>指定JournalNode在本地磁盘存放数据的位置</description>
  </property>
  
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
    <description>设置隔离机制</description>
  </property>
  
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
     <description>使用sshfence隔离机制时需要ssh免登陆</description>
  </property>
  
  <property>
    <name>dfs.data.dir</name>
    <value>/usr/cluster/hadoop3.0-alpha2/dfsdata</value>
  </property>
  
  <property>
    <name>dfs.datanode.socket.write.timeout</name>
    <value>6000000</value>
    <description>hdfs写入的超时设置</description>
  </property>
  
  <property>
    <name>dfs.socket.timeout</name>
    <value>3000000</value>
    <description>hdfs写入的超时设置</description>
  </property>

  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>

</configuration>

```

#### 配置yarn-site.xml

```
<configuration>

<!-- Site specific YARN configuration properties -->

  <!-- NodeManager上运行的附属服务，需配置成mapreduce_shuffle才可运行MapReduce程序 -->  
  <property>  
    <name>yarn.nodemanager.aux-services</name>  
    <value>mapreduce_shuffle</value>  
  </property>  
  
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <value>/usr/cluster/hadoop3.0-alpha2/yarn/nmlocaldir</value>
  </property>

  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/usr/cluster/hadoop3.0-alpha2/yarn/nmlogdirs</value>
  </property>


  <!-- 开启日志 -->  
  <property>  
    <name>yarn.log-aggregation-enable</name>  
    <value>true</value>  
  </property>  
  
  <!-- 配置日志删除时间为14天，-1为禁用，单位为秒 -->  
  <property>  
    <name>yarn.log-aggregation.retain-seconds</name>  
    <value>1209600</value>  
  </property>  
  
  <!-- 当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚集功能时有效），修改为保存的日志文件夹。 -->  
  <property>  
    <name>yarn.nodemanager.remote-app-log-dir</name>  
    <value>/AppLogs</value>  
  </property>    
    
  <!-- //////////////以下为YARN HA的配置////////////// -->  
  <!-- 开启YARN HA -->  
  <property>  
    <name>yarn.resourcemanager.ha.enabled</name>  
    <value>true</value>  
  </property>  
  
  <!-- 启用自动故障转移 -->  
  <property>  
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>  
    <value>true</value>  
  </property>  
  
  <!-- 指定YARN HA的名称 -->  
  <property>  
    <name>yarn.resourcemanager.cluster-id</name>  
    <value>yarncluster</value>  
  </property>  
  
  <!-- 指定两个resourcemanager的名称 -->  
  <property>  
    <name>yarn.resourcemanager.ha.rm-ids</name>  
    <value>rm1,rm2</value>  
  </property>  
  
  <!-- 配置rm1，rm2的主机 -->  
  <property>  
    <name>yarn.resourcemanager.hostname.rm1</name>  
    <value>master1</value>  
  </property>  
  <property>  
    <name>yarn.resourcemanager.hostname.rm2</name>  
    <value>master2</value>  
  </property>  
  
  <!-- 配置YARN的http端口 -->  
  <property>  
    <name>yarn.resourcemanager.webapp.address.rm1</name>  
    <value>master1:8088</value>  
  </property>   
  <property>  
    <name>yarn.resourcemanager.webapp.address.rm2</name>  
    <value>master2:8088</value>  
  </property>  
  
  <!-- 配置zookeeper的地址 -->  
  <property>  
    <name>yarn.resourcemanager.zk-address</name>  
    <value>master1:2181,master2:2181,slave1:2181</value>  
  </property>  
  
  <!-- 配置zookeeper的存储位置 -->  
  <property>  
    <name>yarn.resourcemanager.zk-state-store.parent-path</name>  
    <value>/rmstore</value>  
  </property>  
  
  <!-- 开启yarn resourcemanager restart -->  
  <property>  
    <name>yarn.resourcemanager.recovery.enabled</name>  
    <value>true</value>  
  </property>  
  
  <!-- 配置resourcemanager的状态存储到zookeeper中 -->  
  <property>  
    <name>yarn.resourcemanager.store.class</name>  
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>  
  </property>  
  
  <!-- 开启yarn nodemanager restart -->  
  <property>  
    <name>yarn.nodemanager.recovery.enabled</name>  
    <value>true</value>  
  </property>  

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>

  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>


  <property>
    <name>yarn.resourcemanager.zk.state-store.address</name>
    <value>master1:2181,master2:2181,slave1:2181</value>
  </property>

  <property>
    <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
    <value>5000</value>
  </property>

  <!-- RM1 configs -->
  <property>
    <name>yarn.resourcemanager.address.rm1</name>
    <value>master1:23140</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.address.rm1</name>
    <value>master1:23130</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.https.address.rm1</name>
    <value>master1:23189</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address.rm1</name>
    <value>master1:23188</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
    <value>master1:23125</value>
  </property>

  <property>
    <name>yarn.resourcemanager.admin.address.rm1</name>
    <value>master1:23141</value>
  </property>

<!-- RM2 configs -->
  <property>
    <name>yarn.resourcemanager.address.rm2</name>
    <value>master2:23140</value>
  </property>



  <property>
    <name>yarn.resourcemanager.scheduler.address.rm2</name>
    <value>master2:23130</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.https.address.rm2</name>
    <value>master2:23189</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address.rm2</name>
    <value>master2:23188</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
    <value>master2:23125</value>
  </property>

  <property>
    <name>yarn.resourcemanager.admin.address.rm2</name>
    <value>master2:23141</value>
  </property>

<!-- Node Manager Configs -->
  <property>
    <description>Address where the localizer IPC is.</description>
    <name>yarn.nodemanager.localizer.address</name>
    <value>0.0.0.0:23344</value>
  </property>

  <property>
    <description>NM Webapp address.</description>
    <name>yarn.nodemanager.webapp.address</name>
    <value>0.0.0.0:23999</value>
  </property>

  <property>
    <name>mapreduce.shuffle.port</name>
    <value>23080</value>
  </property>

  <!-- 配置nodemanager IPC的通信端口 -->  
  <property>  
    <name>yarn.nodemanager.address</name>  
    <value>0.0.0.0:45454</value>  
  </property>  
</configuration>

```

#### 启动集群
启动zookeeper，在master1,master2,slave1
```
zkServer.sh start
```

启动journalnode，在master1,master2,slave1
```
hdfs --daemon start journalnode
```

格式化namenode
```
hdfs namenode -format
```

格式化zookeeper
```
hdfs zkfc -formatZK
```

启动namenode，在master1
```
hdfs --daemon start namenode
```

启动zkfc，在master1
```
hdfsd --daemon start zkfc
```

同步数据，在master2
```
hdfs namenode -bootstrapStandby
```

启动namenode，在master2
```
hdfs --daemon start namenode
```

启动zkfc，在master2
```
hdfsd --daemon start zkfc
```

启动hdfs
```
start-dfs.sh
```

启动yarn
```
start-yarn.sh
```