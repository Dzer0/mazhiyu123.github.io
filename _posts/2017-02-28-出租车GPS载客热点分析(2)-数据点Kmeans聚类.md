---
published: true
layout: post
title: 出租车GPS载客热点分析(2)-数据点Kmeans聚类
category: Hadoop
tags: 
  - Hadoop
time: 2017.02.28 15:43:00
excerpt: Kmeans聚类算法是聚类算法的一种，并且是最简单的，应用范围很广泛的聚类算法。从出来过程来看Kmeans算法是自上而下的，从大样本聚类为K个小样本。样本聚类的依据就是数据点之间的距离。
---

 得到出租车载客的上车地点之后，可以进行初步的数据分析,载客点记录的数据格式    

载客点ID | 经度 | 维度
---------|------|-----
1 | 104.126265 | 30.650542
2 | 104.109529 | 30.670981
3 | 104.100514 | 30.683655
4 | 104.132750 | 30.626605
5 | 104.057351 | 30.692118
...|

1. 出租车载客点热点分析  
    通过对载客热点记录GPS经纬度的解析，可以得到载客点的热力图  
    下图是45万个上车点产生的热力图
    ![image](http://od4ghyr10.bkt.clouddn.com/%E4%B8%8A%E8%BD%A6%E8%BD%BD%E5%AE%A2%E7%82%B9%E7%83%AD%E5%8A%9B%E5%9B%BE-45%E4%B8%87.png)
    
### Kmeans聚类分析  
Kmeans聚类算法是聚类算法的一种，并且是最简单的，应用范围很广泛的聚类算法。从出来过程来看Kmeans算法是自上而下的，从大样本聚类为K个小样本。样本聚类的依据就是数据点之间的距离。

##### Kmeans优缺点
1. 优点
    原理简单，可并行化，速度快。
2. 缺点
    初始聚类点和结果有关系，计算之前必须确定K的大小。

##### 复杂度
1. 空间复杂度
```math
O(Inkm)
```
2. 时间复杂度
```math
O(nm)
```
其中m为每个元素字段个数，n为数据量，I为迭代个数，k为聚类中心点个数  
一般I,k,m均可认为是常量，所以时间和空间复杂度可以简化为O(n)，即线性的。

##### 算法基本思想
1. 在数据集中随机选取K个点作为聚类中心点
2. 计算数据集中的每个点和聚类中心K个点的距离，并将数据点划分给距离最短的那个聚类中心
3. 计算每个聚类中心点对应的数据点的平均值，并将计算后的平均值作为新的聚类中心点
4. 最后重复上述两个步骤（2,3）直到两次计算得到的聚类中心点达到设定的收敛条件
5. 得到聚类结果 

![image](http://od4ghyr10.bkt.clouddn.com/Kmeans%E8%81%9A%E7%B1%BB%E6%B5%81%E7%A8%8B%E5%9B%BE.png)  

##### 常用的距离度量
1. 欧式距离  
最简单常用的度量计算方式，即数据点中各个维度只差的平方和最后在开平方。
```math
    d = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2}
```

2. 平方欧式距离  
就是上面欧式距离的平方  
```math
d = (a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2
```

3. 曼哈顿距离  
    它是数据点各个维度之差的绝对值的和
```math
d = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|
```

4. 余弦距离  
余弦距离的计算是基于向量的，考虑的是向量间的角度θ。通过用1减去余弦值得到数值越大距离越远的含义。余弦距离忽略了向量长度，因此并不适用于长度中包含有价值信息的数据集
```math
d = 1 - \frac{(a_1b_1 + a_2b_2 + ... + a_nb_n)}{\sqrt{a_1^2 + a_2^2 + ... + a_n^2}\sqrt{b_1^2 + b_2^2 + ... + b_n^2}}
```

5. 谷本距离  
它可以同时体现两个点之间的距离和夹角
```math
d = 1 - \frac{(a_1b_1 + a_2b_2 + ... + a_nb_n)}{\sqrt{a_1^2 + a_2^2 + ... + a_n^2}\sqrt{b_1^2 + b_2^2 + ... + b_n^2} - (a_1b_1 + a_2b_2 + ... + a_nb_n)}
```

##### 收敛条件
通过比较连续两次计算的聚类中心之间的差值来决定是否收敛。当两次计算的差值小于某个设定的值或者为0的时候结束聚类划分，输出聚类结果
这里选择使用稍复杂的差值计算方法，但本质上依然是插值计算
```math
d = (\frac{(a_1 - b_1)}{a_1 + b_1})^2,,,,
```
##### 空簇的处理 


### 实现

##### 聚类中心数据集的选取

在载客点数据集中随机选取100个数据点作为初始聚类中心
```
sort -R getindataspot | head -100 >> KmeansCenter.txt
```
上面shel命令是将getindataspot数据集中的数据通过sort命令的-R参数随机排序，然后将前100个数据保存到KmeansCenter.txt 文本中
通过命令
```
cat KmeansCenter.txt | wc -l
```
验证 KmeansCenter.txt中数据有随机选取的一百行


##### Mapper过程
数据输入
载客点ID | 纬度 | 经度 | 
---------|------|------|
1|104.126265|30.650542
2|104.109529|30.670981
3|104.100514|30.683655
4|104.132750|30.626605
5|104.057351|30.692118
6|104.059651|30.691794
7|103.996712|30.608103
8|104.002031|30.632534
9|104.066190|30.568210
10|104.063678|30.599831


中间数据输出   (当聚类收敛时，不会进行Reduce阶段，这个数据作为最终的输出结果)
聚类中心ID | 纬度 | 经度 | 
---------|------|------|
1|104.126265|30.650542
1|104.109529|30.670981
1|104.100514|30.683655
2|104.132750|30.626605
2|104.057351|30.692118
3|104.059651|30.691794
3|103.996712|30.608103
3|104.002031|30.632534
3|104.066190|30.568210
3|104.063678|30.599831


Mapper实现  
```
public class KmeansMapper extends Mapper<LongWritable, Text, IntWritable, Text>{
	
	public List<ArrayList<Double>> centers = new ArrayList<ArrayList<Double>>();
	
	protected void setup(Context context) throws IOException {
		// 文件系统的选择
		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(context.getConfiguration());
        
        // 路径暂时写死
		Path inPath = new Path("hdfs://ns1/practice/Kmeans/KmeansCenterData/KmeansCenter.txt");
		FSDataInputStream fsInput = hdfs.open(inPath);
		LineReader lineInput = new LineReader(fsInput, conf);
		Text line = new Text();
		
		while(lineInput.readLine(line) > 0) {
			ArrayList<Double> centerList = new ArrayList<Double>();
			String[] centerInfo = line.toString().split(",");
			centerList.add(Double.parseDouble(centerInfo[1]));
			centerList.add(Double.parseDouble(centerInfo[2]));
			centers.add(centerList);
		}
		lineInput.close();
	}
	
	public Double computeDist(double currentLongitude, double currentLatitude, ArrayList<Double> centers) { 
		double centerLongitude = centers.get(0);
		double centerLatitude = centers.get(1);
		double diffLongitude = Math.pow((currentLongitude - centerLongitude), 2);
		double diffLatitude = Math.pow((currentLatitude - centerLatitude), 2);
		double euclDist = Math.sqrt(diffLongitude + diffLatitude);
		return euclDist;
	}
	
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		//
		String[]  coordinateInfo = value.toString().split(",");
		double currentLongitude = Double.parseDouble(coordinateInfo[1]);
		double currentLatitude = Double.parseDouble(coordinateInfo[2]);
		double minDist = Double.MAX_VALUE;
		int centerIndex = centers.size();
		
		
		for (int i = 0; i < centers.size(); i++) {
			Double currentDist = computeDist(currentLongitude, currentLatitude, centers.get(i));
			if (currentDist < minDist) {
				minDist = currentDist;
				centerIndex = (i+1);
			}
		}
		
		context.write(new IntWritable(centerIndex), new Text(currentLongitude + "," + currentLatitude));
	}
}

```

##### Redcuer过程
结果数据输出(输出的是新的聚类中心)
聚类中心ID | 经度 | 维度
-----------|------|-----
1|104.126265|30.650542
2|104.109529|30.670981
3|104.100514|30.683655
4|104.132750|30.626605
5|104.057351|30.692118

Reducer实现  
```
public class KmeansReducer extends Reducer<IntWritable, Text, Text, NullWritable>{

	@Override
	protected void reduce(IntWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
		
		int centerIndexNum = 0;
		Double[] coorSum = new Double[]{0.0, 0.0}; 
		for (Text value : values) {
			String[] coorInfo = value.toString().split(",");
			for(int i = 0; i < coorInfo.length; i++) {
				coorSum[i] += Double.parseDouble(coorInfo[i]);
			}
			centerIndexNum++;
		}
		
		StringBuilder outKey = new StringBuilder(key.toString());
		for(int i = 0; i < coorSum.length; i++) {
			coorSum[i] /= centerIndexNum;
			outKey.append(",");
			outKey.append(String.valueOf(coorSum[i]));
		}
		
		context.write(new Text(outKey.toString()), NullWritable.get());
		
	}
	
}
```

#### Driver启动类
Driver启动类实现
```
public class KmeansDriver extends Configured implements Tool {

	public static void main(String[] args) throws Exception {
		ToolRunner.run(new Configuration(), new KmeansDriver(), args);
	}
	
	@Override
	public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException  {
		int i = 0;
		while (true) {
			runJob(args, false);
			System.out.println("第" + (++i) + "次聚类计算");
			if(isFinished("hdfs://ns1/practice/Kmeans/KmeansCenterData/KmeansCenter.txt", args[1], 0.0, args[0])) {
				break;
			}
		}
		return runJob(args, true);
	}
	
	public  int runJob(String[] args, boolean isLastRun) throws IOException, ClassNotFoundException, InterruptedException {
		Configuration conf = getConf();
		Job job = Job.getInstance(conf);
		
		job.setJobName("KmeansDriver");
		job.setJarByClass(KmeansDriver.class);
		
		job.setMapperClass(KmeansMapper.class);
		
		job.setMapOutputKeyClass(IntWritable.class);
		job.setMapOutputValueClass(Text.class);
		
		//根据最终聚类中心文件， 最后将数据集分类，此时不需要Reduce
		if (!isLastRun) {
			job.setReducerClass(KmeansReducer.class);
			job.setOutputKeyClass(Text.class);
			job.setOutputValueClass(NullWritable.class);
		}
		
		
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		return job.waitForCompletion(true) ? 0 : 1;
	}
	
	// 是否收敛比较函数
	public static boolean isFinished(String oldCenterDataPath, String newCenterDataPath, Double convergence, String dataInputPath) throws IOException {
		ArrayList<ArrayList<Double>>	oldCenterDataList = getCenterDataFromHDFS(oldCenterDataPath, false);
		ArrayList<ArrayList<Double>>	newCenterDataList = getCenterDataFromHDFS(newCenterDataPath, true);
		
		// 处理出现空簇的情况
		if (oldCenterDataList.size() != newCenterDataList.size()) {
			newCenterDataList = fixEmptyClusters(newCenterDataList, newCenterDataPath, dataInputPath, oldCenterDataList.size());
		}
		
		// 聚类中心点个数
		int centerNum = newCenterDataList.size();
		// 计算的维度 这里是两个，经度和纬度
		int dimensionNum = newCenterDataList.get(0).size();
		double distance = 0.0;
		for(int i = 0; i < centerNum; i++) {
			for(int j = 0; j < dimensionNum; j++) {
				double oldData = Math.abs(oldCenterDataList.get(i).get(j));
				double newData = Math.abs(newCenterDataList.get(i).get(j));
				distance += Math.pow((oldData - newData) / (oldData + newData), 2);
			}
		}
		//convergence:收敛  
		if(distance <= convergence) {
			// 收敛，得到最终的聚类中心点，删除输出目录，准备最后一次聚类划分
			delOutDir(newCenterDataPath);
			return true;
		} else {
			//没有收敛，在聚类中心文件目录下：删除老的聚类中心文件，将新的聚类中心文件复制到这个目录下，删除输出目录
			Configuration conf = new Configuration();
			FileSystem hdfs = FileSystem.get(conf);
			FileStatus[] fileList = hdfs.listStatus(new Path(newCenterDataPath));
			for (int i = 0;i < fileList.length; i++){
				FSDataInputStream in =hdfs.open(fileList[i].getPath());
				FSDataOutputStream out = hdfs.create(new Path(oldCenterDataPath));
				IOUtils.copyBytes(in, out, 4096, true);
			}			
			delOutDir(newCenterDataPath);
		}
		return false;
	}
	
	/* 修复出项空簇的情况
	 * 两种处理方式
	 * 一种是：选择一个距离当前任何质心最远的点，这将消除当前对总平方误差影响最大的点
	 * 另一种处理方式是：从平方差最大的簇中随机选择一个聚类中心替代出现空簇的地方，
	 * 选择一个距离当前任何质心最远的点，这将消除当前对总平方误差影响最大的点
	 */
	public static ArrayList<ArrayList<Double>> fixEmptyClusters(ArrayList<ArrayList<Double>> newCenterDataList,String newCenterDataPath, String dataInputPath, int oldCenterLength) throws IOException {
		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(conf);
		Path newCenterPath = new Path(newCenterDataPath);
		FileStatus[] fileList = hdfs.listStatus(newCenterPath);
		ArrayList<Integer> centerIDList = new ArrayList<Integer>();
		ArrayList<Integer> emptyCenterIDList = new ArrayList<Integer>();
		
		// 得到新聚类中心的ID列表
		for(int i = 0; i < fileList.length; i++) {
			Path centerPath = fileList[i].getPath();
			FSDataInputStream inStream = hdfs.open(centerPath);
			Text lineInfo = new Text();
			LineReader lineReader = new LineReader(inStream, conf);
			while(lineReader.readLine(lineInfo) > 0) {
				String[] coordInfo = lineInfo.toString().split(",");
				centerIDList.add(Integer.parseInt(coordInfo[0]));
			}
		}

		// 找到空簇的ID
		for (int i = 1; i < oldCenterLength; i++) {
			if (centerIDList.get(i-1) != i) {
				emptyCenterIDList.add(i);
			}
		}
		
		if(centerIDList.get(centerIDList.size()-1) != (oldCenterLength)) {
			emptyCenterIDList.add(oldCenterLength);
		}
		
		//这里的空簇处理方式并不严格
		ArrayList<Double> tmpList = new ArrayList<Double>();
		tmpList.add(104.113321);
		tmpList.add(30.670769);
		
		for(int i = 0; i < emptyCenterIDList.size(); i++) {
			newCenterDataList.set(emptyCenterIDList.get(i) - 1, tmpList);
		}
		
		return newCenterDataList;
	}
	
	// 删除作业输出目录，（新聚类中心的目录）
	public static void delOutDir(String delPath) throws IOException {
		Configuration conf = new Configuration();
		FileSystem hdfs = FileSystem.get(conf);
		hdfs.delete(new Path(delPath), true);
		
	}

	/* 从HDFS上读取聚类中心文件解析为List
	 * 可以使用这个函数读取旧的聚类中心数据和新的聚类中心数据，但是读取数据的时候
	 * 旧的聚类文件路径具体到了文件名hdfs://ns1/practice/Kmeans/KmeansCenterData/KmeansCenter.txt
	 * 新的聚类文件路径只是具体到了目录名hdfs://192.168.102.10:9000/practice/Kmeans/KmeansTestData/output/
	 * 因此在读取数据的时候需要判断时候是目录
	 */
	public static ArrayList<ArrayList<Double>> getCenterDataFromHDFS(String centerDataPath, boolean isReadNewCenter) throws IOException {
		ArrayList<ArrayList<Double>> result = new ArrayList<ArrayList<Double>>();
		Configuration conf = new Configuration();
		Path path = new Path(centerDataPath);
		FileSystem hdfs = path.getFileSystem(conf);
		System.out.println("从HDFS上读取聚类中心文件解析为List-openPath");
		
		if(isReadNewCenter) {
			FileStatus[] fileList = hdfs.listStatus(path);
			for(int i = 0; i < fileList.length; i++) {
				result.addAll(getCenterDataFromHDFS(fileList[i].getPath().toString(), false));
			}
			return result;
		}
		FSDataInputStream inputStream = hdfs.open(path);
		LineReader lineReader = new LineReader(inputStream, conf);
		Text lineInfo = new Text();
		while(lineReader.readLine(lineInfo) > 0) {
			String[] coordInfo = lineInfo.toString().split(",");
			ArrayList<Double> coorList = new ArrayList<Double>();
			coorList.add(Double.parseDouble(coordInfo[1]));
			coorList.add(Double.parseDouble(coordInfo[2]));
			result.add(coorList);
		}
		lineReader.close();
		return result;
	}

}
```
